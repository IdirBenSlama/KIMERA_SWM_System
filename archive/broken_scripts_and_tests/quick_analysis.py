#!/usr/bin/env python3
"""
Quick Analysis of Optimized Kimera SWM System
"""

import os
import sys
import time
import requests
from sqlalchemy import create_engine, text
from datetime import datetime

# Initialize structured logger
from backend.utils.kimera_logger import get_system_logger
logger = get_system_logger(__name__)


# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://kimera:kimera_secure_pass_2025@localhost:5432/kimera_swm")
API_BASE_URL = "http://localhost:8000"

def analyze_system():
    """Quick system analysis"""
    logger.info("üöÄ Kimera SWM Quick Analysis")
    logger.info("=" * 40)
    
    engine = create_engine(DATABASE_URL)
    
    with engine.connect() as conn:
        # 1. Database basics
        logger.info("\nüìä Database Status:")
        try:
            version = conn.execute(text("SELECT version()")).scalar()
            logger.info(f"   ‚úÖ PostgreSQL: {version.split()
            
            pgvector = conn.execute(text("""
                SELECT extversion FROM pg_extension WHERE extname = 'vector'
            """)).scalar()
            logger.info(f"   ‚úÖ pgvector: {pgvector}")
            
            db_size = conn.execute(text("""
                SELECT pg_size_pretty(pg_database_size(current_database()))
            """)).scalar()
            logger.info(f"   üì¶ Database size: {db_size}")
        except Exception as e:
            logger.error(f"   ‚ùå Database error: {e}")
        
        # 2. Data counts
        logger.info("\nüìã Data Inventory:")
        tables = ["geoids", "scars", "insights", "multimodal_groundings", "causal_relationships"]
        for table in tables:
            try:
                count = conn.execute(text(f"SELECT COUNT(*) FROM {table}")).scalar()
                logger.info(f"   {table}: {count} records")
            except Exception as e:
                logger.error(f"   {table}: Error - {e}")
        
        # 3. JSONB optimization check
        logger.debug("\nüîß Schema Optimizations:")
        try:
            jsonb_count = conn.execute(text("""
                SELECT COUNT(*) FROM information_schema.columns 
                WHERE data_type = 'jsonb'
            """)).scalar()
            logger.info(f"   ‚úÖ JSONB columns: {jsonb_count}")
            
            # Test JSONB query
            start_time = time.time()
            jsonb_test = conn.execute(text("""
                SELECT COUNT(*) FROM geoids WHERE symbolic_state ? 'symbols'
            """)).scalar()
            jsonb_time = time.time() - start_time
            logger.info(f"   ‚úÖ JSONB query: {jsonb_time:.3f}s")
        except Exception as e:
            logger.error(f"   ‚ùå JSONB error: {e}")
        
        # 4. Vector coverage
        logger.debug("\nüîç Vector Search:")
        try:
            geoid_vectors = conn.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(semantic_vector) as with_vectors
                FROM geoids
            """)).fetchone()
            logger.info(f"   Geoids: {geoid_vectors[1]}/{geoid_vectors[0]} vectors ({geoid_vectors[1]/geoid_vectors[0]*100:.1f}%)
            
            scar_vectors = conn.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(scar_vector) as with_vectors
                FROM scars
            """)).fetchone()
            logger.info(f"   SCARs: {scar_vectors[1]}/{scar_vectors[0]} vectors ({scar_vectors[1]/scar_vectors[0]*100:.1f}%)
            
            # Test vector similarity
            start_time = time.time()
            similarity_test = conn.execute(text("""
                SELECT COUNT(*) FROM (
                    SELECT s1.scar_id, s2.scar_id
                    FROM scars s1, scars s2
                    WHERE s1.scar_vector IS NOT NULL 
                        AND s2.scar_vector IS NOT NULL
                        AND s1.scar_id < s2.scar_id
                    ORDER BY s1.scar_vector <=> s2.scar_vector
                    LIMIT 10
                ) t
            """)).scalar()
            vector_time = time.time() - start_time
            logger.info(f"   ‚úÖ Vector similarity: {vector_time:.3f}s")
        except Exception as e:
            logger.error(f"   ‚ùå Vector error: {e}")
        
        # 5. Index check
        logger.info("\nüìà Indexes:")
        try:
            index_count = conn.execute(text("""
                SELECT COUNT(*) FROM pg_indexes 
                WHERE schemaname = 'public' AND indexname LIKE 'idx_%'
            """)).scalar()
            logger.info(f"   ‚úÖ Custom indexes: {index_count}")
        except Exception as e:
            logger.error(f"   ‚ùå Index error: {e}")
        
        # 6. Materialized views
        logger.info("\nüìã Analytics:")
        try:
            view_count = conn.execute(text("""
                SELECT COUNT(*) FROM pg_matviews WHERE schemaname = 'public'
            """)).scalar()
            logger.info(f"   ‚úÖ Materialized views: {view_count}")
            
            if view_count > 0:
                # Test view query
                start_time = time.time()
                view_test = conn.execute(text("""
                    SELECT COUNT(*) FROM mv_scar_patterns
                """)).scalar()
                view_time = time.time() - start_time
                logger.info(f"   ‚úÖ View query: {view_time:.3f}s ({view_test} patterns)
        except Exception as e:
            logger.error(f"   ‚ùå View error: {e}")
        
        # 7. Functions
        logger.debug("\nüîß Functions:")
        try:
            func_count = conn.execute(text("""
                SELECT COUNT(*) FROM pg_proc 
                WHERE proname IN ('find_similar_scars', 'find_related_geoids')
            """)).scalar()
            logger.info(f"   ‚úÖ Similarity functions: {func_count}")
        except Exception as e:
            logger.error(f"   ‚ùå Function error: {e}")
    
    # 8. API test
    logger.info("\nüåê API Status:")
    try:
        response = requests.get(f"{API_BASE_URL}/system/health", timeout=5)
        logger.info(f"   ‚úÖ Health endpoint: {response.status_code}")
        
        response = requests.get(f"{API_BASE_URL}/system/status", timeout=5)
        logger.info(f"   ‚úÖ Status endpoint: {response.status_code}")
    except Exception as e:
        logger.error(f"   ‚ùå API error: {e}")
    
    # 9. Performance summary
    logger.info("\n‚ö° Performance Summary:")
    logger.info("   ‚úÖ Database: PostgreSQL with pgvector")
    logger.info("   ‚úÖ Schema: JSONB optimized")
    logger.info("   ‚úÖ Search: Vector similarity enabled")
    logger.info("   ‚úÖ Analytics: Materialized views active")
    logger.info("   ‚úÖ API: Endpoints responding")
    
    logger.info("\nüéâ System Analysis Complete!")
    logger.info("\nüìà Key Improvements:")
    logger.info("   ‚Ä¢ Migrated from SQLite to PostgreSQL")
    logger.info("   ‚Ä¢ Implemented vector similarity search")
    logger.info("   ‚Ä¢ Optimized JSON storage with JSONB")
    logger.info("   ‚Ä¢ Created performance indexes")
    logger.info("   ‚Ä¢ Added analytics capabilities")
    logger.info("   ‚Ä¢ Enabled semantic search functions")

def main():
    """Run quick analysis"""
    try:
        analyze_system()
        return 0
    except Exception as e:
        logger.error(f"‚ùå Analysis failed: {e}")
        return 1

if __name__ == "__main__":
    exit(main())