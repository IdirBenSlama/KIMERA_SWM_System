#!/usr/bin/env python3
"""
Optimization Summary for Kimera SWM
Final summary of all optimization improvements
"""

import os
import sys
from sqlalchemy import create_engine, text
import json
from datetime import datetime

# Initialize structured logger
from backend.utils.kimera_logger import get_system_logger
logger = get_system_logger(__name__)


# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://kimera:kimera_secure_pass_2025@localhost:5432/kimera_swm")

def analyze_optimizations():
    """Analyze the impact of all optimizations"""
    logger.info("üöÄ Kimera SWM Optimization Summary")
    logger.info("=" * 50)
    
    engine = create_engine(DATABASE_URL)
    
    with engine.connect() as conn:
        # Check JSONB conversion
        logger.info("\nüìä Database Schema Optimizations:")
        jsonb_columns = conn.execute(text("""
            SELECT table_name, column_name, data_type
            FROM information_schema.columns 
            WHERE data_type = 'jsonb'
            ORDER BY table_name, column_name
        """)).fetchall()
        
        logger.info(f"   ‚úÖ Converted {len(jsonb_columns)
        for col in jsonb_columns:
            logger.info(f"      - {col[0]}.{col[1]}")
        
        # Check indexes
        logger.info("\nüìà Index Optimizations:")
        indexes = conn.execute(text("""
            SELECT indexname, tablename, indexdef
            FROM pg_indexes 
            WHERE schemaname = 'public'
                AND (indexname LIKE 'idx_%' OR indexname LIKE '%vector%')
            ORDER BY tablename, indexname
        """)).fetchall()
        
        logger.info(f"   ‚úÖ Created {len(indexes)
        for idx in indexes:
            logger.info(f"      - {idx[0]} on {idx[1]}")
        
        # Check materialized views
        logger.info("\nüìã Materialized Views:")
        views = conn.execute(text("""
            SELECT matviewname, definition
            FROM pg_matviews
            WHERE schemaname = 'public'
        """)).fetchall()
        
        logger.info(f"   ‚úÖ Created {len(views)
        for view in views:
            logger.info(f"      - {view[0]}")
        
        # Check functions
        logger.debug("\nüîç Vector Search Functions:")
        functions = conn.execute(text("""
            SELECT proname, prosrc
            FROM pg_proc 
            WHERE proname IN ('find_similar_scars', 'find_related_geoids')
        """)).fetchall()
        
        logger.info(f"   ‚úÖ Created {len(functions)
        for func in functions:
            logger.info(f"      - {func[0]}()
        
        # Check data quality
        logger.info("\nüìä Data Quality Metrics:")
        
        # Vector coverage
        vector_stats = conn.execute(text("""
            SELECT 
                'geoids' as table_name,
                COUNT(*) as total_rows,
                COUNT(semantic_vector) as rows_with_vectors,
                ROUND(COUNT(semantic_vector) * 100.0 / COUNT(*), 1) as coverage_pct
            FROM geoids
            UNION ALL
            SELECT 
                'scars' as table_name,
                COUNT(*) as total_rows,
                COUNT(scar_vector) as rows_with_vectors,
                ROUND(COUNT(scar_vector) * 100.0 / COUNT(*), 1) as coverage_pct
            FROM scars
        """)).fetchall()
        
        for stat in vector_stats:
            logger.info(f"   ‚úÖ {stat[0]}: {stat[2]}/{stat[1]} vectors ({stat[3]}% coverage)
        
        # Database size
        db_size = conn.execute(text("""
            SELECT pg_size_pretty(pg_database_size(current_database())) as db_size
        """)).scalar()
        
        logger.info(f"   üì¶ Database size: {db_size}")
        
        # Performance test
        logger.info("\n‚ö° Performance Tests:")
        
        # Test vector similarity query
        import time
        start_time = time.time()
        similar_test = conn.execute(text("""
            SELECT s1.scar_id, s2.scar_id, 
                   1 - (s1.scar_vector <=> s2.scar_vector) as similarity
            FROM scars s1, scars s2
            WHERE s1.scar_vector IS NOT NULL 
                AND s2.scar_vector IS NOT NULL
                AND s1.scar_id < s2.scar_id
            ORDER BY s1.scar_vector <=> s2.scar_vector
            LIMIT 10
        """)).fetchall()
        vector_query_time = time.time() - start_time
        
        logger.info(f"   ‚úÖ Vector similarity query: {vector_query_time:.3f}s ({len(similar_test)
        
        # Test JSONB query
        start_time = time.time()
        jsonb_test = conn.execute(text("""
            SELECT geoid_id, symbolic_state->'symbols' as symbols
            FROM geoids 
            WHERE symbolic_state ? 'symbols'
            LIMIT 10
        """)).fetchall()
        jsonb_query_time = time.time() - start_time
        
        logger.info(f"   ‚úÖ JSONB query: {jsonb_query_time:.3f}s ({len(jsonb_test)
        
        # Test materialized view
        start_time = time.time()
        mv_test = conn.execute(text("""
            SELECT * FROM mv_scar_patterns LIMIT 10
        """)).fetchall()
        mv_query_time = time.time() - start_time
        
        logger.info(f"   ‚úÖ Materialized view query: {mv_query_time:.3f}s ({len(mv_test)
        
        # Summary statistics
        logger.info("\nüìà Optimization Impact Summary:")
        
        total_indexes = len(indexes)
        total_functions = len(functions)
        total_views = len(views)
        total_jsonb = len(jsonb_columns)
        
        logger.info(f"   üéØ Schema Improvements:")
        logger.info(f"      - {total_jsonb} columns optimized with JSONB")
        logger.info(f"      - {total_indexes} performance indexes created")
        logger.info(f"      - {total_views} materialized views for analytics")
        logger.info(f"      - {total_functions} similarity search functions")
        
        logger.info(f"   ‚ö° Performance Gains:")
        logger.info(f"      - Vector queries: ~{vector_query_time:.0f}ms response time")
        logger.info(f"      - JSONB queries: ~{jsonb_query_time:.0f}ms response time")
        logger.info(f"      - Analytics views: ~{mv_query_time:.0f}ms response time")
        
        logger.debug(f"   üîç Search Capabilities:")
        logger.info(f"      - Semantic similarity search enabled")
        logger.info(f"      - SCAR resolution suggestions implemented")
        logger.info(f"      - Graph analytics functions available")
        
        # Test the similarity functions
        logger.info("\nüß™ Testing Similarity Functions:")
        
        # Test find_similar_scars
        test_scar = conn.execute(text("""
            SELECT scar_id FROM scars 
            WHERE scar_vector IS NOT NULL 
            LIMIT 1
        """)).scalar()
        
        if test_scar:
            similar_scars = conn.execute(text("""
                SELECT * FROM find_similar_scars(:scar_id, 3)
            """), {"scar_id": test_scar}).fetchall()
            logger.info(f"   ‚úÖ find_similar_scars()
        
        # Test find_related_geoids
        test_geoid = conn.execute(text("""
            SELECT geoid_id FROM geoids 
            WHERE semantic_vector IS NOT NULL 
            LIMIT 1
        """)).scalar()
        
        if test_geoid:
            related_geoids = conn.execute(text("""
                SELECT * FROM find_related_geoids(:geoid_id, 3)
            """), {"geoid_id": test_geoid}).fetchall()
            logger.info(f"   ‚úÖ find_related_geoids()
        
        logger.info("\nüéâ Optimization Complete!")
        logger.info("\nüìù Key Benefits Achieved:")
        logger.info("   1. ‚úÖ Faster JSON queries with JSONB")
        logger.info("   2. ‚úÖ Efficient vector similarity search")
        logger.info("   3. ‚úÖ Optimized indexes for common queries")
        logger.info("   4. ‚úÖ Materialized views for analytics")
        logger.info("   5. ‚úÖ Similarity-based SCAR resolution")
        logger.info("   6. ‚úÖ Graph analytics capabilities")
        logger.info("   7. ‚úÖ Automated maintenance procedures")
        
        logger.info("\nüöÄ Next Steps:")
        logger.info("   1. Monitor query performance over time")
        logger.info("   2. Refresh materialized views regularly")
        logger.info("   3. Use similarity functions for SCAR resolution")
        logger.info("   4. Leverage graph analytics for insights")
        logger.info("   5. Run maintenance scripts periodically")

def main():
    """Run optimization summary"""
    try:
        analyze_optimizations()
    except Exception as e:
        logger.error(f"‚ùå Error analyzing optimizations: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())